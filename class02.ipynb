{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二讲：神经网络优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 神经网络复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 时间复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 空间复杂度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 学习率策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 指数衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow API: tf.keras.optimizers.schedules.ExponentialDecay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 分段常数衰减"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow API: tf.optimizers.schedules.PiecewiseConstantDecay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Leaky ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 建议"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 均方误差损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 交叉熵损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 自定义损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 欠拟合与过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.1 vanilla SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.2 SGD with Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1.3 SGD with Mesterov Accelerration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 AdaDelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 优化器选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.7 优化算法的常用tricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附录：常用 TensorFlow 及代码实现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学习率策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch ,w is 2.600000, loss is 36.000000, lr is 0.200000\n",
      "After 1 epoch ,w is 1.174400, loss is 12.959999, lr is 0.198000\n",
      "After 2 epoch ,w is 0.321948, loss is 4.728015, lr is 0.196020\n",
      "After 3 epoch ,w is -0.191126, loss is 1.747547, lr is 0.194060\n",
      "After 4 epoch ,w is -0.501926, loss is 0.654277, lr is 0.192119\n",
      "After 5 epoch ,w is -0.691392, loss is 0.248077, lr is 0.190198\n",
      "After 6 epoch ,w is -0.807611, loss is 0.095239, lr is 0.188296\n",
      "After 7 epoch ,w is -0.879339, loss is 0.037014, lr is 0.186413\n",
      "After 8 epoch ,w is -0.923874, loss is 0.014559, lr is 0.184549\n",
      "After 9 epoch ,w is -0.951691, loss is 0.005795, lr is 0.182703\n",
      "After 10 epoch ,w is -0.969167, loss is 0.002334, lr is 0.180876\n",
      "After 11 epoch ,w is -0.980209, loss is 0.000951, lr is 0.179068\n",
      "After 12 epoch ,w is -0.987226, loss is 0.000392, lr is 0.177277\n",
      "After 13 epoch ,w is -0.991710, loss is 0.000163, lr is 0.175504\n",
      "After 14 epoch ,w is -0.994591, loss is 0.000069, lr is 0.173749\n",
      "After 15 epoch ,w is -0.996452, loss is 0.000029, lr is 0.172012\n",
      "After 16 epoch ,w is -0.997660, loss is 0.000013, lr is 0.170292\n",
      "After 17 epoch ,w is -0.998449, loss is 0.000005, lr is 0.168589\n",
      "After 18 epoch ,w is -0.998967, loss is 0.000002, lr is 0.166903\n",
      "After 19 epoch ,w is -0.999308, loss is 0.000001, lr is 0.165234\n",
      "After 20 epoch ,w is -0.999535, loss is 0.000000, lr is 0.163581\n",
      "After 21 epoch ,w is -0.999685, loss is 0.000000, lr is 0.161946\n",
      "After 22 epoch ,w is -0.999786, loss is 0.000000, lr is 0.160326\n",
      "After 23 epoch ,w is -0.999854, loss is 0.000000, lr is 0.158723\n",
      "After 24 epoch ,w is -0.999900, loss is 0.000000, lr is 0.157136\n",
      "After 25 epoch ,w is -0.999931, loss is 0.000000, lr is 0.155564\n",
      "After 26 epoch ,w is -0.999952, loss is 0.000000, lr is 0.154009\n",
      "After 27 epoch ,w is -0.999967, loss is 0.000000, lr is 0.152469\n",
      "After 28 epoch ,w is -0.999977, loss is 0.000000, lr is 0.150944\n",
      "After 29 epoch ,w is -0.999984, loss is 0.000000, lr is 0.149434\n",
      "After 30 epoch ,w is -0.999989, loss is 0.000000, lr is 0.147940\n",
      "After 31 epoch ,w is -0.999992, loss is 0.000000, lr is 0.146461\n",
      "After 32 epoch ,w is -0.999994, loss is 0.000000, lr is 0.144996\n",
      "After 33 epoch ,w is -0.999996, loss is 0.000000, lr is 0.143546\n",
      "After 34 epoch ,w is -0.999997, loss is 0.000000, lr is 0.142111\n",
      "After 35 epoch ,w is -0.999998, loss is 0.000000, lr is 0.140690\n",
      "After 36 epoch ,w is -0.999999, loss is 0.000000, lr is 0.139283\n",
      "After 37 epoch ,w is -0.999999, loss is 0.000000, lr is 0.137890\n",
      "After 38 epoch ,w is -0.999999, loss is 0.000000, lr is 0.136511\n",
      "After 39 epoch ,w is -0.999999, loss is 0.000000, lr is 0.135146\n",
      "After 40 epoch ,w is -1.000000, loss is 0.000000, lr is 0.133794\n",
      "After 41 epoch ,w is -1.000000, loss is 0.000000, lr is 0.132456\n",
      "After 42 epoch ,w is -1.000000, loss is 0.000000, lr is 0.131132\n",
      "After 43 epoch ,w is -1.000000, loss is 0.000000, lr is 0.129821\n",
      "After 44 epoch ,w is -1.000000, loss is 0.000000, lr is 0.128522\n",
      "After 45 epoch ,w is -1.000000, loss is 0.000000, lr is 0.127237\n",
      "After 46 epoch ,w is -1.000000, loss is 0.000000, lr is 0.125965\n",
      "After 47 epoch ,w is -1.000000, loss is 0.000000, lr is 0.124705\n",
      "After 48 epoch ,w is -1.000000, loss is 0.000000, lr is 0.123458\n",
      "After 49 epoch ,w is -1.000000, loss is 0.000000, lr is 0.122223\n",
      "After 50 epoch ,w is -1.000000, loss is 0.000000, lr is 0.121001\n",
      "After 51 epoch ,w is -1.000000, loss is 0.000000, lr is 0.119791\n",
      "After 52 epoch ,w is -1.000000, loss is 0.000000, lr is 0.118593\n",
      "After 53 epoch ,w is -1.000000, loss is 0.000000, lr is 0.117407\n",
      "After 54 epoch ,w is -1.000000, loss is 0.000000, lr is 0.116233\n",
      "After 55 epoch ,w is -1.000000, loss is 0.000000, lr is 0.115071\n",
      "After 56 epoch ,w is -1.000000, loss is 0.000000, lr is 0.113920\n",
      "After 57 epoch ,w is -1.000000, loss is 0.000000, lr is 0.112781\n",
      "After 58 epoch ,w is -1.000000, loss is 0.000000, lr is 0.111653\n",
      "After 59 epoch ,w is -1.000000, loss is 0.000000, lr is 0.110537\n",
      "After 60 epoch ,w is -1.000000, loss is 0.000000, lr is 0.109431\n",
      "After 61 epoch ,w is -1.000000, loss is 0.000000, lr is 0.108337\n",
      "After 62 epoch ,w is -1.000000, loss is 0.000000, lr is 0.107254\n",
      "After 63 epoch ,w is -1.000000, loss is 0.000000, lr is 0.106181\n",
      "After 64 epoch ,w is -1.000000, loss is 0.000000, lr is 0.105119\n",
      "After 65 epoch ,w is -1.000000, loss is 0.000000, lr is 0.104068\n",
      "After 66 epoch ,w is -1.000000, loss is 0.000000, lr is 0.103027\n",
      "After 67 epoch ,w is -1.000000, loss is 0.000000, lr is 0.101997\n",
      "After 68 epoch ,w is -1.000000, loss is 0.000000, lr is 0.100977\n",
      "After 69 epoch ,w is -1.000000, loss is 0.000000, lr is 0.099967\n",
      "After 70 epoch ,w is -1.000000, loss is 0.000000, lr is 0.098968\n",
      "After 71 epoch ,w is -1.000000, loss is 0.000000, lr is 0.097978\n",
      "After 72 epoch ,w is -1.000000, loss is 0.000000, lr is 0.096998\n",
      "After 73 epoch ,w is -1.000000, loss is 0.000000, lr is 0.096028\n",
      "After 74 epoch ,w is -1.000000, loss is 0.000000, lr is 0.095068\n",
      "After 75 epoch ,w is -1.000000, loss is 0.000000, lr is 0.094117\n",
      "After 76 epoch ,w is -1.000000, loss is 0.000000, lr is 0.093176\n",
      "After 77 epoch ,w is -1.000000, loss is 0.000000, lr is 0.092244\n",
      "After 78 epoch ,w is -1.000000, loss is 0.000000, lr is 0.091322\n",
      "After 79 epoch ,w is -1.000000, loss is 0.000000, lr is 0.090409\n",
      "After 80 epoch ,w is -1.000000, loss is 0.000000, lr is 0.089505\n",
      "After 81 epoch ,w is -1.000000, loss is 0.000000, lr is 0.088610\n",
      "After 82 epoch ,w is -1.000000, loss is 0.000000, lr is 0.087724\n",
      "After 83 epoch ,w is -1.000000, loss is 0.000000, lr is 0.086846\n",
      "After 84 epoch ,w is -1.000000, loss is 0.000000, lr is 0.085978\n",
      "After 85 epoch ,w is -1.000000, loss is 0.000000, lr is 0.085118\n",
      "After 86 epoch ,w is -1.000000, loss is 0.000000, lr is 0.084267\n",
      "After 87 epoch ,w is -1.000000, loss is 0.000000, lr is 0.083424\n",
      "After 88 epoch ,w is -1.000000, loss is 0.000000, lr is 0.082590\n",
      "After 89 epoch ,w is -1.000000, loss is 0.000000, lr is 0.081764\n",
      "After 90 epoch ,w is -1.000000, loss is 0.000000, lr is 0.080946\n",
      "After 91 epoch ,w is -1.000000, loss is 0.000000, lr is 0.080137\n",
      "After 92 epoch ,w is -1.000000, loss is 0.000000, lr is 0.079336\n",
      "After 93 epoch ,w is -1.000000, loss is 0.000000, lr is 0.078542\n",
      "After 94 epoch ,w is -1.000000, loss is 0.000000, lr is 0.077757\n",
      "After 95 epoch ,w is -1.000000, loss is 0.000000, lr is 0.076979\n",
      "After 96 epoch ,w is -1.000000, loss is 0.000000, lr is 0.076209\n",
      "After 97 epoch ,w is -1.000000, loss is 0.000000, lr is 0.075447\n",
      "After 98 epoch ,w is -1.000000, loss is 0.000000, lr is 0.074693\n",
      "After 99 epoch ,w is -1.000000, loss is 0.000000, lr is 0.073946\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "\n",
    "epoch = 100\n",
    "LR_BASE = 0.2 # 最初学习率\n",
    "LR_DECAY = 0.99 # 学习率衰减率\n",
    "LR_STEP = 1 # 喂入多少轮BATCH_SIZE 后，更新一次学习率\n",
    "for epoch in range(epoch): # for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环100次迭代。\n",
    "    lr = LR_BASE * LR_DECAY ** (epoch / LR_STEP)\n",
    "    with tf.GradientTape() as tape: # with结构到grads框起了梯度的计算过程。\n",
    "        loss = tf.square(w + 1)\n",
    "    grads = tape.gradient(loss, w) # .gradient函数告知谁对谁求导\n",
    "    \n",
    "    w.assign_sub(lr * grads)  # .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads\n",
    "    print(\"After %s epoch ,w is %f, loss is %f, lr is %f\" % (epoch, w.numpy(), loss, lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m无法执行代码，已释放会话。请尝试重新启动内核。"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "N = 100\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "0.5,\n",
    "decay_steps=10,\n",
    "decay_rate=0.9,\n",
    "staircase=False)\n",
    "y = []\n",
    "for global_step in range(N):\n",
    "    lr = lr_schedule(global_step)\n",
    "    y.append(lr)\n",
    "x = range(N)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(x, y, 'r-')\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Learning Rate')\n",
    "plt.title('ExponentialDecay')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 其它"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.cast\n",
    "转换数据（张量）类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 2], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "x = tf.constant([1.8, 2.2], dtype=tf.float32)\n",
    "print(tf.cast(x, tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.random.normal\n",
    "生成服从正态分布的随机值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 5), dtype=float32, numpy=\n",
       "array([[-0.06236406,  0.17700054,  1.7307663 ,  0.99779886, -0.94573677],\n",
       "       [ 0.5334611 , -0.11980411, -1.006372  , -1.3327197 , -1.8125427 ],\n",
       "       [ 0.22657698, -0.22983427, -0.8714001 ,  0.6743138 ,  1.4302018 ]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.random.normal([3, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.where\n",
    "根据condition，取x或y中的值。如果为True，对应位置取x的值；如果为False，对应位置取y的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int32, numpy=array([1, 6, 3, 8])>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.where([True, False, True, False], [1, 2, 3, 4], [5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=int32, numpy=array([1, 2, 3, 4, 5])>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant([1, 2, 3 ,1, 1])\n",
    "b = tf.constant([0, 1, 3, 4, 5])\n",
    "c = tf.where(tf.greater(a, b), a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 0.417022004702574\n",
      "b: [[7.20324493e-01 1.14374817e-04 3.02332573e-01]\n",
      " [1.46755891e-01 9.23385948e-02 1.86260211e-01]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rdm = np.random.RandomState(seed=1)\n",
    "a = rdm.rand()\n",
    "b = rdm.rand(2, 3)\n",
    "print(\"a:\", a)\n",
    "print(\"b:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### vstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: [[1 2 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "c = np.vstack((a, b))\n",
    "print(\"c:\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### mgrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[1. 1. 1. 1.]\n",
      " [2. 2. 2. 2.]]\n",
      "y: [[2.  2.5 3.  3.5]\n",
      " [2.  2.5 3.  3.5]]\n",
      "x.ravel(): [1. 1. 1. 1. 2. 2. 2. 2.]\n",
      "y.ravel(): [2.  2.5 3.  3.5 2.  2.5 3.  3.5]\n",
      "grid: [[1.  2. ]\n",
      " [1.  2.5]\n",
      " [1.  3. ]\n",
      " [1.  3.5]\n",
      " [2.  2. ]\n",
      " [2.  2.5]\n",
      " [2.  3. ]\n",
      " [2.  3.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 生成等间隔数值点\n",
    "x, y = np.mgrid[1:3:1, 2:4:0.5]\n",
    "# 将x, y拉直，并合并配对为二维张量，生成二维坐标点\n",
    "grid = np.c_[x.ravel(), y.ravel()]\n",
    "print(\"x:\", x)\n",
    "print(\"y:\", y)\n",
    "print(\"x.ravel():\", x.ravel())\n",
    "print(\"y.ravel():\", y.ravel())\n",
    "print(\"grid:\", grid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netsun",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
